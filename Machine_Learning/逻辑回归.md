# 逻辑回归

其实我们并不需要确切地算出概率密度，算出决策边界足矣。

|  名称   |                             模型                             |                           损失函数                           |                         参数更新方程                         |
| :-----: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 感知机  |                 $$g(x)=sign(w^{T}x+w_{0})$$                  |          $$\ell(w,b)=-r^{(\ell)}(w^Tx^{(\ell)}+b)$$          | $$\boldsymbol{w} \leftarrow \boldsymbol{w}+\eta r^{(\ell)} x^{(\ell)}$$if $$\begin{gathered}r^{(\ell)} g_{\mathrm{i}}\left(x^{(\ell)}\right) \leq 0\end{gathered}$$ |
| Sigmoid | $$y=\operatorname{sigmoid}\left(w^{T}x+w_{0}\right)\\=\frac{1}{1+\exp \left[-\left(w^{T} x+w_{0}\right)\right]}$$ | $$\begin{aligned}&\ell\left(w, w_{0} \mid x, r\right)=-r \log y-(1-r) \log (1-y) \\&L\left(w, w_{0} \mid D\right)=-\sum_{l=1}^{N} r^{(l)} \log y^{(l)}+\left(1-r^{(l)}\right) \log \left(1-y^{(l)}\right)\end{aligned}$$ | $$\boldsymbol{w}\leftarrow\boldsymbol{w}+\sum_{l=1}^{N}\left(r^{(l)}-y^{(l)}\right) x_{j}^{(l)}$$ |
| SoftMax | $$y_{i}=\operatorname{softmax}\left(\boldsymbol{w}_{\mathrm{i}}^{\mathrm{T}} x+w_{\mathrm{i} 0}\right)\\=\frac{\exp \left[\boldsymbol{w}_{i}{ }^{T} x+w_{i 0}\right]}{\sum_{j=1}^{K} \exp \left[\boldsymbol{w}_{j}{ }^{T} x+w_{j 0}\right]} \quad(i=1, . ., K)$$ | $$\begin{aligned}&L(\boldsymbol{w} \mid \boldsymbol{x}, \boldsymbol{r})=-\sum_{i=1}^{K} r_{i} \log y_{i} \\&L(\boldsymbol{w} \mid D)=-\sum_{l=1}^{N}\left[\sum_{i=1}^{K} r_{i}^{(l)} \log y_{i}^{(l)}\right]\end{aligned}$$ | $$\boldsymbol{w}\leftarrow\boldsymbol{w}+\sum_{l=1}^{N}\left(r^{(l)}-y^{(l)}\right) x_{j}^{(l)}$$ |

### 分析

+ 感知机：

  + 对训练数据集要求较高，只能处理线性可分的数据集，并且解不唯一，存在多个解。

  + 0-1这样的离散值不能精确表示一个实例有多接近于某个类$$C_i$$

+ sigmoid可能导致梯度消失
+ 普通的线性分类器只能数据处理线性可分的情况。(对于线性不可分的数据必须使用核方法或者诸如神经网络等非线性模型)
+ 即使线性可分，普通的线性分类器得到的解是不唯一的。

